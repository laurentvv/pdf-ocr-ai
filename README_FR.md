<div align="center">

# üìÑ PDF to Markdown OCR

[![GitHub](https://img.shields.io/badge/GitHub-Repository-333333?logo=github)](https://github.com/laurentvv/pdf-to-md-ocr)
[![Python](https://img.shields.io/badge/Python-3.13+-blue?logo=python)](https://www.python.org/downloads/)
[![uv](https://img.shields.io/badge/uv-Modern%20package%20manager-00a8ff?logo=python)](https://github.com/astral-sh/uv)

**Convertit des documents PDF en Markdown en utilisant les capacit√©s OCR aliment√©es par plusieurs fournisseurs d'IA**

*Aliment√© par des mod√®les IA pour une extraction de texte pr√©cise et la reconnaissance d'√©l√©ments UI*

</div>

---

## ‚ú® Fonctionnalit√©s

<div align="center">

| Fonctionnalit√© | Description |
|--------|-------------|
| üß† **OCR aliment√© par IA** | Extrait le texte des documents PDF en utilisant des mod√®les IA avanc√©s |
| üîç **Support multi-format** | Traite les PDF textuels et scann√©s |
| üíª **Reconnaissance √©l√©ments UI** | Identifie et d√©crit les √©l√©ments d'interface dans les captures d'√©cran |
| üìù **Sortie structur√©e** | G√©n√®re du Markdown structur√© en pr√©servant la hi√©rarchie du document |
| üìÑ **Gestion multi-pages** | G√®re les PDF multipages avec traitement individuel des pages |
| üìä **Suivi de progression** | Suivi en temps r√©el avec pourcentage et ETA |
| ‚ö° **M√©triques de performance** | Calculs de temps et d'analyse de performance |
| üîÑ **Support multi-fournisseur** | Fonctionne avec LM Studio, Ollama et llama.cpp |

</div>

## Pr√©requis

### Pr√©requis de base
- **Python 3.13+** (requis pour les m√©thodes d'installation traditionnelles)
- **Un des fournisseurs d'IA suivants** :
  - **LM Studio** ex√©cut√© localement avec un mod√®le de vision (ex. `qwen/qwen3-vl-30b`)
  - **Ollama** ex√©cut√© localement avec un mod√®le de vision (ex. `llava`)
  - **llama.cpp** ex√©cut√© localement avec un mod√®le de vision
- **Mat√©riel** : RAM et VRAM suffisants pour le traitement des PDF (recommand√© : 16 Go+ de RAM pour les documents volumineux)

### Pr√©requis sp√©cifiques √† l'installation

#### Pour les m√©thodes bas√©es sur uv (recommand√©) :
- **Gestionnaire de paquets uv** install√© (installez avec : `pip install uv`)

#### Pour les m√©thodes traditionnelles :
- **Gestionnaire de paquets pip**
- Git pour cloner le d√©p√¥t (si vous clonez)

## üöÄ Installation

Pour la meilleure exp√©rience, nous recommandons d'utiliser les m√©thodes bas√©es sur uv. Ces approches offrent une meilleure gestion des d√©pendances et une utilisation plus simple :

### ü•á Option 1 : Ex√©cution directe avec uvx (Recommand√©e - Aucune Installation Requise)

Ex√©cutez l'outil directement depuis le d√©p√¥t git sans aucune installation locale. C'est la fa√ßon la plus simple d'utiliser l'outil :

<div align="center">

```bash
uvx --from git+https://github.com/laurentvv/pdf-to-md-ocr pdf-ocr-ai <input.pdf> <output.md>
```

</div>

<details>
<summary><b>Pourquoi cette approche ?</b></summary>

- ‚úÖ Aucune installation locale requise
- ‚úÖ Utilise toujours la derni√®re version
- ‚úÖ R√©solution automatique des d√©pendances
- ‚úÖ Aucun conflit avec d'autres projets Python
- ‚úÖ Parfait pour une utilisation ponctuelle

</details>

---

### ü•à Option 2 : Installation en tant qu'outil avec uv (Recommand√©e pour une utilisation r√©guli√®re)

Cette m√©thode installe d√©finitivement l'outil dans votre environnement, le rendant disponible comme utilitaire en ligne de commande.

<div align="center">

```bash
uv tool install git+https://github.com/laurentvv/pdf-to-md-ocr
```

</div>

<details>
<summary><b>Avantages de cette approche</b></summary>

- ‚úÖ La commande `pdf-ocr-ai` devient disponible globalement
- ‚úÖ uv g√®re automatiquement les d√©pendances dans un environnement isol√©
- ‚úÖ Pas besoin de r√©installer √† chaque fois que vous utilisez l'outil
- ‚úÖ Meilleure isolation des d√©pendances qu'avec pip traditionnel
- ‚úÖ Facilit√© de mise √† jour ou de suppression de l'outil
- ‚úÖ Parfait pour une utilisation r√©guli√®re

</details>

<div align="center">

**Utilisation apr√®s installation :**
```bash
pdf-ocr-ai <input.pdf> <output.md>
```

</div>

#### Commandes de gestion de l'outil

<div align="center">

| Commande | Description |
|--------|-------------|
| `uv tool install git+https://github.com/laurentvv/pdf-to-md-ocr` | Installer l'outil |
| `uv tool install --force-reinstall git+https://github.com/laurentvv/pdf-to-md-ocr` | Mettre √† jour l'outil |
| `uv tool uninstall pdf-ocr-ai` | Supprimer l'outil |

</div>

---

### ü•â Option 3 : Installation traditionnelle (Pour le d√©veloppement)

Seulement recommand√©e si vous pr√©voyez de modifier le code ou de travailler avec un environnement virtuel :

1. Clonez le d√©p√¥t et installez les d√©pendances requises depuis pyproject.toml :
   ```bash
   pip install .
   ```
   OU avec uv :
   ```bash
   uv sync
   ```

## üìã Utilisation

<div align="center">

Pour la meilleure exp√©rience, nous recommandons d'utiliser les m√©thodes bas√©es sur uv :

### üéØ Commande principale (uvx - Aucune Installation Requise)
```bash
uvx --from git+https://github.com/laurentvv/pdf-to-md-ocr pdf-ocr-ai <input.pdf> <output.md> [options]
```

### üß∞ Outil install√© (Apr√®s `uv tool install`)
```bash
pdf-ocr-ai <input.pdf> <output.md> [options]
```

### üîÑ Compatibilit√© descendante
L'outil prend √©galement en charge l'ancien nom de commande pour la compatibilit√© descendante :
```bash
pdf-ocr-lmstudio <input.pdf> <output.md> [options]
```

</div>

---

### ‚öôÔ∏è Options de ligne de commande

<div align="center">

| Option | Description | Valeur par d√©faut |
|--------|-------------|------------------|
| `--provider` | Fournisseur d'IA √† utiliser : lm-studio, ollama, llama.cpp | `lm-studio` |
| `--provider-url` | URL personnalis√©e du fournisseur (d√©pend du fournisseur) | Voir d√©tails ci-dessous |
| `--model <nom_du_modele>` | Sp√©cifier le mod√®le √† utiliser avec le fournisseur | `qwen/qwen3-vl-30b` |
| `--dpi <valeur>` | D√©finir le DPI pour la conversion d'image | `300` |

</div>

---

### üí° Exemples

<div align="center">

#### D√©marrage rapide avec LM Studio
```bash
# Ex√©cution directe avec uvx (pas d'installation n√©cessaire)
uvx --from git+https://github.com/laurentvv/pdf-to-md-ocr pdf-ocr-ai document.pdf output.md
```

#### Utilisation d'Ollama
```bash
# Avec le fournisseur Ollama
uvx --from git+https://github.com/laurentvv/pdf-to-md-ocr pdf-ocr-ai document.pdf output.md --provider ollama --model llava

# Avec une URL Ollama personnalis√©e
uvx --from git+https://github.com/laurentvv/pdf-to-md-ocr pdf-ocr-ai document.pdf output.md --provider ollama --provider-url http://localhost:11434/v1 --model llava
```

#### Utilisation de llama.cpp
```bash
# Avec le fournisseur llama.cpp
uvx --from git+https://github.com/laurentvv/pdf-to-md-ocr pdf-ocr-ai document.pdf output.md --provider "llama.cpp" --model qwen2-vl
```

#### Utilisation avanc√©e
```bash
# Avec DPI personnalis√©
uvx --from git+https://github.com/laurentvv/pdf-to-md-ocr pdf-ocr-ai document.pdf output.md --provider ollama --model llava --dpi 200

# Utilisation de l'ancien nom de commande (compatibilit√© descendante)
pdf-ocr-lmstudio document.pdf output.md --provider ollama --model llava
```

#### Avec l'outil install√©
```bash
# Apr√®s installation via l'outil uv
pdf-ocr-ai document.pdf output.md --provider ollama --model llava
```

</div>

## Configuration des fournisseurs d'IA

### Configuration de LM Studio

1. T√©l√©chargez et installez [LM Studio](https://lmstudio.ai/)
2. Dans LM Studio, t√©l√©chargez un mod√®le de vision (recommand√© : `qwen/qwen3-vl-30b`)
3. D√©marrez le serveur local avec le mod√®le charg√© :
   - Ouvrez LM Studio
   - S√©lectionnez votre mod√®le de vision dans la liste des mod√®les
   - Cliquez sur le bouton "Load" pour charger le mod√®le
   - Cliquez sur le bouton "Start Server" pour d√©marrer le serveur API local
4. Le script se connectera automatiquement √† l'API √† l'adresse `http://localhost:1234/v1`
5. Assurez-vous que l'option "Enable remote access (allows external connections)" est d√©coch√©e pour une utilisation locale

### Configuration d'Ollama

1. T√©l√©chargez et installez [Ollama](https://ollama.ai/)
2. T√©l√©chargez un mod√®le de vision :
   ```bash
   ollama pull llava
   # ou
   ollama pull qwen2-vl
   ```
3. D√©marrez Ollama (g√©n√©ralement ex√©cut√© automatiquement apr√®s installation) :
   ```bash
   ollama serve
   ```
4. Le script se connectera √† l'API √† l'adresse `http://localhost:11434/v1`

### Configuration de llama.cpp

1. Clonez et compilez [llama.cpp](https://github.com/ggerganov/llama.cpp)
2. Compilez avec le support serveur :
   ```bash
   make
   cd examples/server
   make server
   ```
3. Ex√©cutez le serveur avec un mod√®le de vision :
   ```bash
   # Commande d'exemple (ajustez les chemins et param√®tres selon vos besoins)
   ./server -m path/to/model.gguf --port 8080
   ```
4. Le script se connectera √† l'API √† l'adresse `http://localhost:8080/v1`

## üî¨ Fonctionnement

<div align="center">

| √âtape | Description |
|------|-------------|
| 1Ô∏è‚É£ | Le script convertit chaque page PDF en une image haute r√©solution (300 DPI) |
| 2Ô∏è‚É£ | Chaque image est envoy√©e au mod√®le de vision du fournisseur d'IA s√©lectionn√© via l'API |
| 3Ô∏è‚É£ | Le mod√®le IA effectue un OCR et identifie les √©l√©ments d'interface dans les images |
| 4Ô∏è‚É£ | Les r√©sultats sont format√©s en Markdown structur√© |
| 5Ô∏è‚É£ | Toutes les pages sont combin√©es en un seul fichier de sortie Markdown |
| 6Ô∏è‚É£ | La progression s'affiche en temps r√©el avec le temps restant estim√© |
| 7Ô∏è‚É£ | Les m√©triques de performance sont calcul√©es et affich√©es √† l'ach√®vement |

</div>

## üìä Suivi de progression

<div align="center">

| Fonctionnalit√© | Description |
|--------|-------------|
| üìà **Barre de progression visuelle** | Affiche le pourcentage achev√© en temps r√©el |
| ‚è≥ **ETA** | Estimation du temps restant |
| ‚è±Ô∏è **Timing par page** | Temps de traitement de chaque page |
| üìâ **Timing moyen** | Temps moyen de traitement par page |
| üìã **R√©sum√© des performances** | M√©triques globales √† l'ach√®vement |

</div>

## Consid√©rations de performance

- Les PDF volumineux (>100 pages) peuvent n√©cessiter une quantit√© substantielle de m√©moire (plusieurs Go)
- Le temps de traitement varie consid√©rablement selon le fournisseur d'IA et le mod√®le utilis√©
- Pour les documents volumineux, envisagez de les traiter par lots ou d'utiliser une machine avec suffisamment de RAM
- La vitesse de traitement d√©pend de la complexit√© du document, du mat√©riel et des performances du fournisseur d'IA
- Les images haute DPI (300 DPI) offrent une meilleure pr√©cision OCR mais prennent plus de temps
- La premi√®re ex√©cution peut √™tre plus lente car les mod√®les sont charg√©s en m√©moire

## Configuration avanc√©e

Le script utilise les param√®tres par d√©faut suivants, qui peuvent √™tre modifi√©s dans le code source :
- Fournisseur par d√©faut : lm-studio
- URL LM Studio : http://localhost:1234/v1
- URL Ollama : http://localhost:11434/v1
- URL llama.cpp : http://localhost:8080/v1
- DPI : 300 (pour la qualit√© de l'image)
- Mod√®le : qwen/qwen3-vl-30b (modifiable en ligne de commande)
- Tokens max : 2048
- D√©lai d'attente : 60 secondes
- Tentatives de r√©essai : 3

## üõ†Ô∏è D√©pannage

<div align="center">

| Probl√®me | Solution |
|-------|----------|
| üîå **Erreurs de connexion API** | Assurez-vous que votre fournisseur d'IA s√©lectionn√© (LM Studio/Ollama/llama.cpp) est en cours d'ex√©cution et accessible |
| ‚ùå **√âchec du traitement** | V√©rifiez que le nom du mod√®le dans la commande correspond √† ce qui est disponible dans votre fournisseur |
| üíæ **Probl√®mes de m√©moire** | Pour les PDF volumineux, assurez-vous d'avoir au moins 1 Go de RAM par 50 pages |
| üß† **Mod√®le non trouv√©** | V√©rifiez que le nom du mod√®le correspond exactement √† ce qui est disponible dans votre fournisseur |
| ‚ö†Ô∏è **Probl√®mes de performance** | Fermez d'autres applications avant de traiter des documents volumineux |
| üö´ **Erreurs de m√©moire** | Essayez de traiter des PDF plus petits ou augmentez les ressources syst√®me |
| üìä **Barre de progression manquante** | Assurez-vous que tqdm est disponible dans votre environnement Python |
| üêç **Probl√®mes d'installation uv** | Assurez-vous que uv est correctement install√© : `pip install uv` |

</div>

## Migration depuis le .venv traditionnel vers uv

Si vous avez un r√©pertoire `.venv` existant et que vous souhaitez passer √† la gestion d'environnement bas√©e sur uv :

1. Sauvegardez votre configuration actuelle si n√©cessaire
2. Conservez votre `.venv` existant si vous souhaitez y revenir plus tard
3. Utilisez plut√¥t les commandes uv :
   ```bash
   uv venv  # Cr√©e un nouvel environnement g√©r√© par uv
   uv sync  # Installe les d√©pendances avec uv
   ```
4. Lors de l'activation des environnements, utilisez les commandes uv ou s√©lectionnez manuellement l'interpr√©teur dans votre IDE

## D√©pendances

- `openai` : Pour la communication API avec les fournisseurs d'IA
- `PyMuPDF` : Pour le traitement PDF et l'extraction d'images
- `tqdm` : Pour la visualisation de la barre de progression